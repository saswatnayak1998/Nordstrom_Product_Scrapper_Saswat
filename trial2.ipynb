{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import csv\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "from selenium_stealth import stealth\n",
    "from proxyscrape import create_collector\n",
    "\n",
    "collector = proxyscrape.create_collector('default', 'http')\n",
    "\n",
    "\n",
    "def get_proxy():\n",
    "    proxy = collector.get_proxy({'country': 'united states'})\n",
    "    if proxy:\n",
    "        return f\"http://{proxy.host}:{proxy.port}\"\n",
    "    return None\n",
    "\n",
    "def get_driver(proxy=None, headless=False):\n",
    "    chrome_options = Options()\n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless\")  \n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument('--disable-gpu')\n",
    "    chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "    chrome_options.add_argument('--start-maximized')\n",
    "    chrome_options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    chrome_options.add_argument(f'user-agent={UserAgent().random}')\n",
    "    \n",
    "    if proxy:\n",
    "        chrome_options.add_argument(f'--proxy-server={proxy}')\n",
    "    \n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)\n",
    "\n",
    "    stealth(driver,\n",
    "            languages=[\"en-US\", \"en\"],\n",
    "            vendor=\"Google Inc.\",\n",
    "            platform=\"Win32\",\n",
    "            webgl_vendor=\"Intel Inc.\",\n",
    "            renderer=\"Intel Iris OpenGL Engine\",\n",
    "            fix_hairline=True,\n",
    "            )\n",
    "\n",
    "    driver.execute_cdp_cmd('Page.addScriptToEvaluateOnNewDocument', {\n",
    "        'source': '''\n",
    "            Object.defineProperty(navigator, 'webdriver', {\n",
    "                get: () => undefined\n",
    "            });\n",
    "            window.navigator.chrome = {\n",
    "                runtime: {},\n",
    "            };\n",
    "            Object.defineProperty(navigator, 'languages', {\n",
    "                get: () => ['en-US', 'en']\n",
    "            });\n",
    "            Object.defineProperty(navigator, 'plugins', {\n",
    "                get: () => [1, 2, 3, 4, 5]\n",
    "            });\n",
    "        '''\n",
    "    })\n",
    "\n",
    "    return driver\n",
    "\n",
    "def write_to_csv(csv_file, headers, data):\n",
    "    with open(csv_file, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=headers)\n",
    "        writer.writerow(data)\n",
    "\n",
    "def scrape_page(page, csv_file, headers, proxy=None, headless=False):\n",
    "    driver = get_driver(proxy, headless)\n",
    "    try:\n",
    "        url = f'https://www.nordstrom.com/browse/women/clothing?breadcrumb=Home%2FWomen%2FClothing&origin=topnav&page={page}'\n",
    "        driver.get(url)\n",
    "\n",
    "        WebDriverWait(driver, 120).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '//article[contains(@class, \"zzWfq RpUx3\")]'))\n",
    "        )\n",
    "\n",
    "        time.sleep(random.uniform(5, 10))\n",
    "\n",
    "        html_content = driver.page_source\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        articles = soup.find_all('article', class_='zzWfq RpUx3')\n",
    "\n",
    "        for article in articles:\n",
    "            try:\n",
    "                name_element = article.find('h3', class_='kKGYj Y9bA4').find('a')\n",
    "                name = name_element.text.strip()\n",
    "                product_url = \"https://www.nordstrom.com\" + name_element['href']\n",
    "\n",
    "                brand_element = article.find('div', class_='KtWqU jgLpg Y9bA4 Io521')\n",
    "                brand = brand_element.text.strip()\n",
    "                \n",
    "                price_element = article.find('span', class_='qHz0a EhCiu dls-ihm460')\n",
    "                price = price_element.text.strip()\n",
    "                \n",
    "                image_element = article.find('img', {'name': 'product-module-image'})\n",
    "                image_url = image_element['src']\n",
    "\n",
    "                star_rating_element = article.find('span', class_='T2Mzf', role='img')\n",
    "                star_rating = star_rating_element['aria-label'].strip() if star_rating_element else 'No rating'\n",
    "                \n",
    "                num_reviews_element = article.find('span', class_='HZv8u')\n",
    "                num_reviews = num_reviews_element.text.strip() if num_reviews_element else 'No reviews'\n",
    "\n",
    "                product = {\n",
    "                    \"Name\": name,\n",
    "                    \"Brand\": brand,\n",
    "                    \"Price\": price,\n",
    "                    \"Image URL\": image_url,\n",
    "                    \"Product URL\": product_url,\n",
    "                    \"Star Rating\": star_rating,\n",
    "                    \"Number of Reviews\": num_reviews\n",
    "                }\n",
    "\n",
    "                write_to_csv(csv_file, headers, product)\n",
    "\n",
    "                print(f\"Page {page} - Name: {name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to scrape a product entry on page {page}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load page {page} with proxy {proxy}: {e}\")\n",
    "        driver.save_screenshot(f'error_page_{page}.png')\n",
    "        return False\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    return True\n",
    "\n",
    "csv_file = 'nordstrom_products_fast_2.csv'\n",
    "\n",
    "headers = [\"Name\", \"Brand\", \"Price\", \"Image URL\", \"Product URL\", \"Star Rating\", \"Number of Reviews\"]\n",
    "\n",
    "def main():\n",
    "    start_page = 500\n",
    "    end_page = 693  # Define the range of pages you want to scrape\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        futures = []\n",
    "        for page in range(start_page, end_page + 1):\n",
    "            proxy = get_proxy()  # Get a new proxy for each page\n",
    "            futures.append(executor.submit(scrape_page, page, csv_file, headers, proxy, True))\n",
    "\n",
    "        for future in as_completed(futures):\n",
    "            if not future.result():\n",
    "                print(\"One of the scraping tasks failed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
